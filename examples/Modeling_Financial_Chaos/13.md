Here is **Chapter 13** of "Modeling Financial Chaos."

This chapter acts as a "Turbocharger" for the model.
In previous chapters, we fed Mamba raw price data. While Mamba is capable of learning the laws of motion from raw positions, it is inefficient. It’s like asking a self-driving car to navigate using only GPS coordinates, without giving it a speedometer or an accelerometer.

For the **Data Scientist**, this chapter focuses on **Feature Engineering**. We will transform a 1-dimensional time series ($Price$) into a multi-dimensional Tensor that explicitly describes the "Physics" of the asset, drastically speeding up convergence.

-----

# Chapter 13: Input Dynamics – Feature Engineering for Phase Spaces

### Beyond Raw Price: Feeding Derivatives, Pressure, and Fractal Dimensions

A State Space Model (SSM) like Mamba is mathematically designed to track a system defined by differential equations:
$$h'(t) = \mathbf{A}h(t) + \mathbf{B}x(t)$$

If we feed it raw prices ($x_t = P_t$), the model has to internally "learn" to calculate velocity and acceleration to understand the trend.
If we **explicitly calculate** these derivatives and feed them as inputs, we remove a massive computational burden from the model. We stop asking the AI to *discover* physics and start asking it to *predict* the future using physics we already know.

This chapter moves our input dimension ($d\_input$) from 1 to 5. We will engineer features representing **Kinematics** (Motion), **Microstructure** (Pressure), and **Geometry** (Roughness).

-----

## 13.1 The Derivative Layer: Kinematics

In classical mechanics, if you know an object's Position, Velocity, and Acceleration, you can predict its location at $t+1$ with high precision.
In finance, these correspond to **Price**, **Returns**, and **Change in Returns**.

### 1\. Velocity (Log Returns)

Price is unbounded (non-stationary). Returns are centered around zero (stationary-ish). This is the "Speed" of the market.
$$v_t = \ln\left(\frac{P_t}{P_{t-1}}\right)$$

### 2\. Acceleration (The "Jerk")

Changes in velocity. This is critical for regime detection. A crash often begins with a spike in negative acceleration *before* the price drops significantly.
$$a_t = v_t - v_{t-1}$$

### 3\. Volatility (Kinetic Energy)

Energy in physics is proportional to $v^2$. In finance, instantaneous variance is the square of returns.
$$E_t = v_t^2$$

**Python Implementation:**

```python
def add_kinematics(df):
    # 1. Velocity (Log Returns)
    df['velocity'] = np.log(df['Close'] / df['Close'].shift(1))
    
    # 2. Acceleration (Delta Velocity)
    df['acceleration'] = df['velocity'].diff()
    
    # 3. Energy (Instantaneous Variance)
    df['energy'] = df['velocity'] ** 2
    
    # Clean NaNs created by shifting
    df.dropna(inplace=True)
    return df
```

**Why Mamba Loves This:**
By feeding `[velocity, acceleration]`, you are essentially giving Mamba the first two terms of a **Taylor Series expansion**. The model can now focus its "brain power" (weights) on learning the higher-order non-linear terms (the chaos) rather than wasting capacity relearning basic calculus.

-----

## 13.2 Microstructure: The "Pressure" Gauge

Kinematics describe *how* price moved. Microstructure describes *why* it moved.
If Price is the car, **Order Flow** is the fuel injection.

We look at the **Limit Order Book (LOB)** to measure the imbalance between buyers and sellers.

### Feature 1: Order Book Imbalance (OBI)

If there are 500 BTC on the Bid side (to buy) and only 50 BTC on the Ask side (to sell), price *must* rise to clear the liquidity.
$$OBI_t = \frac{V_{bid} - V_{ask}}{V_{bid} + V_{ask}}$$

  * **Range:** -1 (Bearish Pressure) to +1 (Bullish Pressure).

### Feature 2: VPIN (Volume-Synchronized Probability of Informed Trading)

This is a measure of **Toxic Flow**.
High VPIN means volume is executing rapidly in one direction (e.g., a massive sell-off). It is a proxy for "Informed Traders" (Insiders/Institutions) dumping positions. Mamba uses this to differentiate between "Retail Noise" (Low VPIN) and "Smart Money Moves" (High VPIN).

```python
def calculate_obi(bids, asks):
    """
    bids: Series of Bid Volumes
    asks: Series of Ask Volumes
    """
    return (bids - asks) / (bids + asks)
```

-----

## 13.3 Fractal Dimensions: The Roughness Sensor

Markets are fractals. A stable market is usually smooth (low dimension). A crashing market is jagged (high dimension).
We can measure this "Roughness" using the **Hurst Exponent ($H$)** on a rolling window.

  * **$H \approx 0.5$:** Random Walk. (Mamba should lower confidence).
  * **$H > 0.5$:** Trending/Persistent. (Mamba should increase position size).
  * **$H < 0.5$:** Mean Reverting. (Mamba should prepare for a reversal).

**The Logic:**
Mamba's $\Delta$ parameter (time discretization) naturally aligns with the Hurst exponent. By feeding $H$ explicitly, we tell the model exactly how "random" the current moment is.

```python
from hurst import compute_Hc

def rolling_hurst(series, window=100):
    """
    Calculates Hurst Exponent on a rolling window.
    Note: This is computationally expensive. Optimization required for HFT.
    """
    hurst_series = []
    for i in range(len(series)):
        if i < window:
            hurst_series.append(0.5) # Default
            continue
            
        slice_ = series[i-window : i]
        H, c, data = compute_Hc(slice_, kind='price', simplified=True)
        hurst_series.append(H)
        
    return np.array(hurst_series)
```

-----

## 13.4 The Input Tensor: Stacking Channels

We now have a rich dataset. We must stack these into a Tensor for PyTorch.
Crucially, **Normalization** is harder now.

  * $OBI$ is bounded [-1, 1].
  * $Price$ is unbounded [0, $\infty$].
  * $Velocity$ is small float [-0.1, 0.1].

We must normalize each channel independently using **Z-Scores**.

**The New Dataset Class:**

```python
class MultiFeatureDataset(Dataset):
    def __init__(self, df, seq_len=64):
        # Features to include
        feature_cols = ['Close', 'velocity', 'acceleration', 'energy', 'obi', 'hurst']
        self.data = df[feature_cols].values
        self.seq_len = seq_len

    def __getitem__(self, idx):
        # Extract window: Shape [Seq_Len, Num_Features]
        window = self.data[idx : idx + self.seq_len]
        
        # Independent Normalization per feature channel
        # Axis 0 = Time. We compute mean/std down the time axis.
        mean = np.mean(window, axis=0)
        std = np.std(window, axis=0) + 1e-8
        
        window_norm = (window - mean) / std
        
        # Return Tensor
        # Shape: [Seq_Len, d_input=6]
        return torch.tensor(window_norm, dtype=torch.float32)
```

### Updating the Mamba Model

When initializing `MambaRegimeModel` in Chapter 5, we must now update the input dimension.

```python
# Old
# model = MambaRegimeModel(d_input=1, ...)

# New
model = MambaRegimeModel(d_input=6, d_model=128, d_state=32)
```

**The Effect on Embedding:**
The first layer of our model is `nn.Linear(d_input, d_model)`.
This linear layer now acts as a **Fusion Layer**. It takes kinematics, pressure, and geometry, and learns how to weight them to produce the initial latent vector.

  * In a Crash, it might learn to weigh `acceleration` and `obi` heavily.
  * In a Bull Market, it might weigh `velocity` and `hurst` heavily.

-----

## 13.5 Summary: High-Octane Fuel

By completing Chapter 13, we have stopped feeding our supercar low-grade fuel.
We have engineered a feature set that respects the physics of the market:

1.  **Kinematics** (Velocity/Acceleration) for Trend.
2.  **OBI** for Liquidity Pressure.
3.  **Hurst** for Regime Roughness.

This rich input allows Mamba to converge faster during training and, more importantly, makes the **Latent State ($h_t$)** much more descriptive when we visualize it in Chapter 6.

We are now ready to take this high-dimensional data and feed it into the **Training Pipeline** (Chapter 5).