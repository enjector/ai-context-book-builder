Here is **Chapter 6** of "Modeling Financial Chaos."

This chapter focuses on **Interpretability**. A common criticism of Deep Learning in finance is that it is a "Black Box." You put data in, you get a prediction out, and you have no idea *why*.
For a Quantitative Researcher, this is unacceptable. You cannot risk millions of dollars on a black box.

In this chapter, we perform "Digital Neuroscience." We will extract the internal neural activations (the Latent States) of the trained Mamba model. We will demonstrate that these vectors, when projected into 3D space, physically reconstruct the **Attractor Wings** we theorized in Chapter 2.

-----

# Chapter 6: Extracting the Ghost in the Machine

### Latent State Visualization and Manifold Reconstruction

We have trained our Mamba model. It has watched 100,000 steps of synthetic volatility. It has learned to minimize the "Physics Loss."
But what has it actually learned?

Has it simply memorized a moving average? Or has it constructed a genuine internal representation of market regimes?

To answer this, we must look at the **Hidden State ($h_t$)**.
In the Mamba equation $h'(t) = \mathbf{A}h(t) + \mathbf{B}x(t)$, the vector $h(t)$ represents the **System State**. It is a compression of all relevant history into a fixed-size vector (e.g., 64 dimensions).

If our hypothesis is correct—that markets are chaotic systems with distinct attractors—then these vectors should not be randomly distributed. They should form distinct **Clusters** or **Manifolds** in the 64-dimensional space. One cluster should represent "Stability," and another should represent "Chaos."

This chapter guides you through extracting these states, reducing their dimensionality with PCA, and proving that the model has learned to separate the regimes.

-----

## 6.1 The Extraction Logic

In standard inference (`model.predict()`), we only care about the final output (Price). Here, we care about the *journey*.
We need to intercept the data flow *after* it leaves the Mamba backbone but *before* it gets squashed by the final prediction head.

**The "Manifold Embedding":**
Strictly speaking, the internal $h_t$ in Mamba is a very large matrix (Batch $\times$ d\_model $\times$ d\_state). For practical regime detection, we use the **Output Feature Vector** of the Mamba block (Size: `d_model`, usually 64 or 128). This vector is the projection of the internal state into the model's "understanding" of the current moment.

### The Code: Feature Extraction Hook

We modify our evaluation loop to discard the price prediction and keep the features.

```python
import numpy as np
import torch
from torch.utils.data import DataLoader
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt

def extract_latent_dynamics(model, dataset, device):
    """
    Feeds data through Mamba and captures the hidden representations.
    """
    model.eval()
    # Use a sequential loader (shuffle=False) to preserve time order
    loader = DataLoader(dataset, batch_size=64, shuffle=False)
    
    all_states = []
    all_timestamps = [] # To track when these states happened
    
    print("Extracting latent states...")
    with torch.no_grad():
        for i, (x, _) in enumerate(loader):
            x = x.to(device)
            
            # Forward pass: Get the 'features' (second output from our class)
            _, features = model(x)
            
            # We only care about the state at the LAST time step of the window
            # Shape: [Batch_Size, Seq_Len, d_model] -> [Batch, d_model]
            last_step_features = features[:, -1, :]
            
            all_states.append(last_step_features.cpu().numpy())
            
    # Concatenate into a single matrix [Total_Samples, d_model]
    # Example: [98000, 64]
    latent_matrix = np.concatenate(all_states, axis=0)
    return latent_matrix

# Execution
# latent_matrix = extract_latent_dynamics(model, dataset_heston, device)
# print(f"Extracted Manifold Shape: {latent_matrix.shape}")
```

-----

## 6.2 The Curse of Dimensionality (and the Cure)

We now have a matrix of shape `[100,000, 64]`.
We cannot visualize 64 dimensions. We need to project this down to 2D or 3D.

**Which Algorithm?**

  * **t-SNE / UMAP:** Great for clustering, but they distort global distances. They might make the regimes look separated even if they aren't.
  * **PCA (Principal Component Analysis):** Linear and deterministic. It rotates the data to find the axes of maximum variance.

For Financial Regimes, **PCA is superior**.
Why? Because the *distance* between points in PCA space is meaningful.

  * If Point A (Today) is far from Point B (Yesterday), the market moved fast (High Velocity).
  * If Point A is far from the origin, the Variance is high.

We want to preserve these "Physics" properties, so we choose PCA.

```python
def reduce_dimensions(latent_matrix, n_components=2):
    print(f"Reducing dimensions from {latent_matrix.shape[1]} to {n_components}...")
    pca = PCA(n_components=n_components)
    
    # Fit and Transform
    latent_2d = pca.fit_transform(latent_matrix)
    
    # Calculate Explained Variance (How much info did we lose?)
    variance = np.sum(pca.explained_variance_ratio_)
    print(f"Retained Information: {variance:.2%}")
    
    return latent_2d

# Execution
# latent_2d = reduce_dimensions(latent_matrix)
```

-----

## 6.3 Visualizing the Attractor: The "Wings" Plot

This is the "Moment of Truth."
We plot the 2D points. Crucially, we color-code them using the **Ground Truth Labels** (0 or 1) from our Heston Generator in Chapter 4.

  * **Hypothesis:** If Mamba has learned nothing, the Blue dots (Stable) and Red dots (Crash) will be mixed together like a soup.
  * **Success:** We should see two distinct "islands" or "wings." The Blue dots should cluster in one region, and the Red dots in another.

<!-- end list -->

```python
def plot_regime_manifold(latent_2d, labels):
    plt.figure(figsize=(10, 8))
    
    # Scatter plot
    # We use alpha=0.5 to see density
    scatter = plt.scatter(
        latent_2d[:, 0], 
        latent_2d[:, 1], 
        c=labels, 
        cmap='coolwarm', 
        alpha=0.5, 
        s=1
    )
    
    plt.title("The Ghost in the Machine: Mamba's Internal Phase Space")
    plt.xlabel("Principal Component 1 (Likely Volatility)")
    plt.ylabel("Principal Component 2 (Likely Momentum)")
    plt.legend(handles=scatter.legend_elements()[0], labels=['Stable', 'Chaos'])
    plt.grid(True, alpha=0.3)
    plt.show()

# Execution
# labels_trimmed = heston_labels[64+1:] # Align labels with windowed data
# plot_regime_manifold(latent_2d, labels_trimmed)
```

### Interpreting the Plot

When you run this on a successfully trained model, you will typically see a **"V" shape** or two blobs connected by a thin bridge.

1.  **The Bridge:** This is the Transition Zone. These points represent the days *during* the regime shift.
2.  **The Separation:** The fact that Red and Blue are separated proves the model has learned to distinguish the "Texture" of the price action. It knows that a -1% move in the Blue region is different from a -1% move in the Red region.

-----

## 6.4 Trajectory Analysis: Watching the Crash Happen

Static plots are good, but markets move. We can animate the trajectory of the last 100 days to see **"The Path to Chaos."**

Imagine a "worm" crawling across the plot.

  * **Days 1-90:** The worm coils tightly in the Blue Cluster (Stable).
  * **Day 91:** The worm stretches out toward the empty space (The Bridge). **Warning Signal.**
  * **Day 95:** The worm crosses the midline.
  * **Day 100:** The worm is now coiling in the Red Cluster. The regime has changed.

This visual is the ultimate tool for a Software Architect to explain the model to a Portfolio Manager.
*"We don't just predict price. We track the position of the market in this map. When the dot moves here, we sell."*

-----

## 6.5 Summary: The Map is Drawn

We have proven that Mamba is not a black box. It is a **Geometry Engine**.

  * It takes raw price noise.
  * It organizes it into a structured Manifold.
  * It clearly separates Stable dynamics from Chaotic dynamics.

We now have a coordinate system for the market. But coordinates alone don't make money. We need an automated way to draw a line in the sand and say, *"If the point crosses this line, liquidate the portfolio."*

In **Chapter 7**, we will implement the **Unsupervised Classifier**. We will use K-Means Clustering to mathematically define the boundaries of these "Wings" so our trading algorithm can make decisions in real-time without human intervention.